# -*- coding: utf-8 -*-
"""8_colab_to_webpage.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16IJAPBzhd-gkH-x_-n83cqYFO-BkgYN1

This notebook (and the slides from lecture 8) will help you go straight from training a model in Colab to deploying it in a webpage with TensorFlow.js - without having to leave the browser.

Configure this notebook to work with your GitHub account by populating these fields.
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import keras
from keras import Sequential
from keras.layers import Flatten, MaxPool2D, Conv2D, Conv1D, GlobalMaxPooling1D, MaxPooling1D ,Dense, LSTM, RNN, Dropout, GRU, Embedding
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

from sklearn.model_selection import train_test_split
# import tensorflowjs

!pip install tensorflowjs

"""As an example, we will create and vectorize a few documents. (Check out https://www.gutenberg.org/ for a bunch of free e-books.)"""

# A few snippets from Alice in Wonderland
ex1 = "Alice was beginning to get very tired of sitting by her sister on the bank."
ex2 = "Once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it."

# Dracula
ex3 = "Buda-Pesth seems a wonderful place."
ex4 = "Left Munich at 8:35 P. M., on 1st May, arriving at Vienna early next morning."

# Illiad
ex5 = "Scepticism was as much the result of knowledge, as knowledge is of scepticism."
ex6 = "To be content with what we at present know, is, for the most part, to shut our ears against conviction."

x_train = [ex1, ex2, ex3, ex4, ex5, ex6]
y_train = [0, 0, 1, 1, 2, 2] # Indicating which book each sentence is from

"""Tokenize the documents, create a word index (word -> number)."""

max_len = 20
num_words = 1000
# Fit the tokenizer on the training data
t = Tokenizer(num_words=num_words)
t.fit_on_texts(x_train)

print(t.word_index)

"""Here's how we vectorize a document."""

vectorized = t.texts_to_sequences([ex1])
print(vectorized)

"""Apply padding if necessary."""

padded = pad_sequences(vectorized, maxlen=max_len, padding='post')

print(padded)

"""We will save the word index in metadata. Later, we'll use it to convert words typed in the browser to numbers for prediction."""

metadata = {
  'word_index': t.word_index,
  'max_len': max_len,
  'vocabulary_size': num_words,
}

"""Define a model."""

embedding_size = 8
n_classes = 3
epochs = 10

model = keras.Sequential()
model.add(keras.layers.Embedding(num_words, embedding_size, input_shape=(max_len,)))
model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(3, activation='softmax'))
model.compile('adam', 'sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

"""Prepare some training data."""

x_train = t.texts_to_sequences(x_train)
x_train = pad_sequences(x_train, maxlen=max_len, padding='post')
print(x_train)

model.fit(x_train, y_train, epochs=epochs)

"""Demo using the model to make predictions."""

test_example = "Left Munich at 8:35 P. M., on 1st May, arriving at Vienna early next morning."
x_test = t.texts_to_sequences([test_example])
x_test = pad_sequences(x_test, maxlen=max_len, padding='post')
print(x_test)

preds = model.predict(x_test)
print(preds)
import numpy as np
print(np.argmax(preds))

!ls

"""All done! Hopefully everything worked. You may need to wait a few moments for the changes to appear in your site. If not working, check the JavaScript console for errors (in Chrome: View -> Developer -> JavaScript Console)."""

print("Now, visit https://%s.github.io/%s/" % (USER_NAME, SITE_NAME))

"""If you are debugging and Chrome is failing to pick up your changes, though you've verified they're present in your GitHub repo, see the second answer to: https://superuser.com/questions/89809/how-to-force-refresh-without-cache-in-google-chrome

## Adding Three books from Gutenberg
"""

from urllib.request import urlopen 

url1 = "http://www.gutenberg.org/files/120/120-0.txt" #Treasure Island
url2 = "http://www.gutenberg.org/files/1184/1184-0.txt" #Count of Mote Cristo
url3 = "http://www.gutenberg.org/cache/epub/1661/pg1661.txt" #Sherlock Holmes

raw1 = urlopen(url1).read().decode('utf-8')
raw2 = urlopen(url2).read().decode('utf-8')
raw3 = urlopen(url3).read().decode('utf-8')

def clean_books(book_url, lines, label):
  
  book = urlopen(book_url).read().decode('utf-8')
  book_clean= book.replace('\r\n', '')
  
  
  x_split = book_clean.split('.')[1000:2000+lines]
  x_train=[]
  
  i=0
  for line in x_split:
    if i>=lines:
      break
    if len(line)>10:
      x_train.append(line)
      i+=1
  
  y_train = list(np.full(lines, label))
  
  return x_train, y_train

x_train1, y_train1= clean_books(url1, 2500, 0)
x_train2, y_train2= clean_books(url2, 2500, 1)
x_train3, y_train3= clean_books(url3, 2500, 2)

len(x_train1)

x_train1.extend(x_train2)
x_train1.extend(x_train3)
y_train1.extend(y_train2)
y_train1.extend(y_train3)

len(y_train1)

x_train, x_test, y_train, y_test = train_test_split(x_train1, y_train1, test_size=0.15, random_state=4356)

max_len = 20
num_words = 2000
# Fit the tokenizer on the training data
t = Tokenizer(num_words=num_words)
t.fit_on_texts(x_train)

x_train = t.texts_to_sequences(x_train)
x_train = pad_sequences(x_train, maxlen=max_len, padding='post')
# print(x_train)

x_test = t.texts_to_sequences(x_test)
x_test = pad_sequences(x_test, maxlen=max_len, padding='post')

"""### Vanilla Model"""

optimizer = keras.optimizers.Adam(lr=0.001)

callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=1)

embedding_size = 16
n_classes = 3
epochs = 30

model = keras.Sequential()
model.add(keras.layers.Embedding(num_words, embedding_size, input_shape=(max_len,)))
model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(3, activation='softmax'))
model.compile(optimizer, 'sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=1)

model.fit(x_train, np.array(y_train), epochs= 50, validation_split=0.15, callbacks=[callback])

model.evaluate(x_test, np.array(y_test))

model.save('books_classification_embedding.h5')

"""#### Vanilla Model Test Accuracy: 79%

### LSTM Model
"""

optimizer = keras.optimizers.Adam(lr= 2*1e-4)

embedding_size = 128
n_classes = 3
epochs = 30

model = keras.Sequential()
model.add(Embedding(num_words, embedding_size, input_shape=(max_len,)))

# model.add(Dropout(0.25))
# model.add(LSTM(64, kernel_initializer='glorot_normal', recurrent_initializer='glorot_normal', activation='relu', return_sequences=True))
# model.add(Dropout(0.3))
model.add(LSTM(64, kernel_initializer='glorot_normal', recurrent_initializer='glorot_normal', activation='sigmoid'))
# model.add(Dropout(0.2))
# model.add(Flatten())
# model.add(Dense(64, activation='relu'))
# model.add(Dense(50, activation='relu'))
# model.add(Dropout(0.3))

# model.add(GRU(64))
# model.add(Dropout(0.3))

model.add(Dense(3, activation='softmax'))
model.compile(optimizer, 'sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=1)

model.fit(x_train, np.array(y_train), epochs= 50, validation_split=0.15, callbacks= [callback], batch_size=32)

model.evaluate(x_test, np.array(y_test))

model.save('books_classification_lstm.h5')

"""#### LSTM test accuracy: 80%

### Model on Browser
"""

# your github username
USER_NAME = "Srinidhi-kv" 

# the email associated with your commits
# (may not matter if you leave it as this)
USER_EMAIL = "sk4356@columbia.edu" 

# the user token you've created (see the lecture 8 slides for instructions)
TOKEN = "e0c2a9d915b1e08557d28b88764f47fa44a84628" 

# site name
# for example, if my user_name is "foo", then this notebook will create
# a site at https://foo.github.io/hw4/
SITE_NAME = "deep_learning_hw4"

!git config --global user.email {USER_NAME}
!git config --global user.name  {USER_EMAIL}

repo_path = USER_NAME + '.github.io'
if not os.path.exists(os.path.join(os.getcwd(), repo_path)):
  !git clone https://{USER_NAME}:{TOKEN}@github.com/{USER_NAME}/{USER_NAME}.github.io

os.chdir(repo_path)
!git pull

project_path = os.path.join(os.getcwd(), SITE_NAME)
if not os.path.exists(project_path): 
  os.mkdir(project_path)
os.chdir(project_path)

# DO NOT MODIFY
MODEL_DIR = os.path.join(project_path, "model_js")
if not os.path.exists(MODEL_DIR):
  os.mkdir(MODEL_DIR)

test_example = ' One of my last thoughts was of the captain, who had so often strode along the beachwith his cocked hat, his sabre-cut cheek, and his old brass telescope'
x_test = t.texts_to_sequences([test_example])
x_test = pad_sequences(x_test, maxlen=max_len, padding='post')

preds = model.predict(x_test)
# print(preds)
print(np.argmax(preds))

metadata = {
  'word_index': t.word_index,
  'max_len': max_len,
  'vocabulary_size': num_words,
}

import json
import tensorflowjs as tfjs

metadata_json_path = os.path.join(MODEL_DIR, 'metadata.json')
json.dump(metadata, open(metadata_json_path, 'wt'))
tfjs.converters.save_keras_model(model, MODEL_DIR)
print('\nSaved model artifcats in directory: %s' % MODEL_DIR)

index_html = """
<!doctype html>

<body>
  <style>
    #textfield {
      font-size: 120%;
      width: 60%;
      height: 200px;
    }
  </style>
  <h1>
    Title
  </h1>
  <hr>
  <div class="create-model">
    <button id="load-model" style="display:none">Load model</button>
  </div>
  <div>
    <div>
      <span>Vocabulary size: </span>
      <span id="vocabularySize"></span>
    </div>
    <div>
      <span>Max length: </span>
      <span id="maxLen"></span>
    </div>
  </div>
  <hr>
  <div>
    <select id="example-select" class="form-control">
      <option value="example1">Treasure Island</option>
      <option value="example2">Count of Monte Carlo</option>
      <option value="example3">Sherlock Holmes</option>
    </select>
  </div>
  <div>
    <textarea id="text-entry"></textarea>
  </div>
  <hr>
  <div>
    <span id="status">Standing by.</span>
  </div>

  <script src='https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js'></script>
  <script src='index.js'></script>
</body>
"""

index_js = """
const HOSTED_URLS = {
  model:
      'model_js/model.json',
  metadata:
      'model_js/metadata.json'
};

const examples = {
  'example1':
      ' One of my lastthoughts was of the captain, who had so often strode along the beachwith his cocked hat, his sabre-cut cheek, and his old brass telescope',
  'example2':
      ' Here is thepaper; do you know the writing?” As he spoke, Villefort drew the letterfrom his pocket, and presented it to Dantès',
  'example3':
      'But how could you guess what the motive was?'      
};

function status(statusText) {
  console.log(statusText);
  document.getElementById('status').textContent = statusText;
}

function showMetadata(metadataJSON) {
  document.getElementById('vocabularySize').textContent =
      metadataJSON['vocabulary_size'];
  document.getElementById('maxLen').textContent =
      metadataJSON['max_len'];
}

function settextField(text, predict) {
  const textField = document.getElementById('text-entry');
  textField.value = text;
  doPredict(predict);
}

function setPredictFunction(predict) {
  const textField = document.getElementById('text-entry');
  textField.addEventListener('input', () => doPredict(predict));
}

function disableLoadModelButtons() {
  document.getElementById('load-model').style.display = 'none';
}

function doPredict(predict) {
  const textField = document.getElementById('text-entry');
  const result = predict(textField.value);
  score_string = "Class scores: ";
  for (var x in result.score) {
    score_string += x + " ->  " + result.score[x].toFixed(3) + ", "
  }
  //console.log(score_string);
  status(
      score_string + ' elapsed: ' + result.elapsed.toFixed(3) + ' ms)');
}

function prepUI(predict) {
  setPredictFunction(predict);
  const testExampleSelect = document.getElementById('example-select');
  testExampleSelect.addEventListener('change', () => {
    settextField(examples[testExampleSelect.value], predict);
  });
  settextField(examples['example1'], predict);
}

async function urlExists(url) {
  status('Testing url ' + url);
  try {
    const response = await fetch(url, {method: 'HEAD'});
    return response.ok;
  } catch (err) {
    return false;
  }
}

async function loadHostedPretrainedModel(url) {
  status('Loading pretrained model from ' + url);
  try {
    const model = await tf.loadModel(url);
    status('Done loading pretrained model.');
    disableLoadModelButtons();
    return model;
  } catch (err) {
    console.error(err);
    status('Loading pretrained model failed.');
  }
}

async function loadHostedMetadata(url) {
  status('Loading metadata from ' + url);
  try {
    const metadataJson = await fetch(url);
    const metadata = await metadataJson.json();
    status('Done loading metadata.');
    return metadata;
  } catch (err) {
    console.error(err);
    status('Loading metadata failed.');
  }
}

class Classifier {

  async init(urls) {
    this.urls = urls;
    this.model = await loadHostedPretrainedModel(urls.model);
    await this.loadMetadata();
    return this;
  }

  async loadMetadata() {
    const metadata =
        await loadHostedMetadata(this.urls.metadata);
    showMetadata(metadata);
    this.maxLen = metadata['max_len'];
    console.log('maxLen = ' + this.maxLen);
    this.wordIndex = metadata['word_index']
  }

  predict(text) {
    // Convert to lower case and remove all punctuations.
    const inputText =
        text.trim().toLowerCase().replace(/(\.|\,|\!)/g, '').split(' ');
    // Look up word indices.
    const inputBuffer = tf.buffer([1, this.maxLen], 'float32');
    for (let i = 0; i < inputText.length; ++i) {
      const word = inputText[i];
      inputBuffer.set(this.wordIndex[word], 0, i);
      //console.log(word, this.wordIndex[word], inputBuffer);
    }
    const input = inputBuffer.toTensor();
    //console.log(input);

    status('Running inference');
    const beginMs = performance.now();
    const predictOut = this.model.predict(input);
    //console.log(predictOut.dataSync());
    const score = predictOut.dataSync();//[0];
    predictOut.dispose();
    const endMs = performance.now();

    return {score: score, elapsed: (endMs - beginMs)};
  }
};

async function setup() {
  if (await urlExists(HOSTED_URLS.model)) {
    status('Model available: ' + HOSTED_URLS.model);
    const button = document.getElementById('load-model');
    button.addEventListener('click', async () => {
      const predictor = await new Classifier().init(HOSTED_URLS);
      prepUI(x => predictor.predict(x));
    });
    button.style.display = 'inline-block';
  }

  status('Standing by.');
}

setup();
"""

with open('index.html','w') as f:
  f.write(index_html)
  
with open('index.js','w') as f:
  f.write(index_js)

!git add . 
!git commit -m "colab -> github"
!git push https://{USER_NAME}:{TOKEN}@github.com/{USER_NAME}/{USER_NAME}.github.io/ master

print("Now, visit https://%s.github.io/%s/" % (USER_NAME, SITE_NAME))

